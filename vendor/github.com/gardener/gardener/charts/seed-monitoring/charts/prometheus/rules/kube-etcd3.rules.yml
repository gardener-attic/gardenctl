groups:
- name: kube-etcd3.rules
  rules:

  # alert if main etcd is down
  - alert: KubeEtcdMainDown
    expr: sum(up{job="kube-etcd3",role="main"}) < 1
    for: 3m
    labels:
      service: etcd
      severity: blocker
      type: seed
    annotations:
      description: etcd3 cluster {{ $labels.role }} is unavailable or cannot be scrapped
      summary: etcd3 main cluster down

  # alert if events etcd is down
  - alert: KubeEtcdEventsDown
    expr: sum(up{job="kube-etcd3",role="events"}) < 1
    for: 3m
    labels:
      service: etcd
      severity: critical
      type: seed
    annotations:
      description: etcd3 cluster {{ $labels.role }} is unavailable or cannot be scrapped
      summary: etcd3 events cluster down

  # etcd leader alerts
  - alert: KubeEtcd3NoLeader
    expr: etcd_server_has_leader{job="kube-etcd3"} == 0
    for: 1m
    labels:
      service: etcd
      severity: critical
      type: seed
    annotations:
      description: etcd3 member {{ $labels.pod }} has no leader
      summary: etcd3 member has no leader

  # alert if there are lots of leader changes
  - alert: KubeEtcd3HighNumberOfLeaderChanges
    expr: increase(etcd_server_leader_changes_seen_total{job="kube-etcd3"}[1h]) > 3
    labels:
      service: etcd
      severity: warning
      type: seed
    annotations:
      description: etcd3 pod {{ $labels.pod }} has seen {{ $value }} leader changes
        within the last hour
      summary: a high number of leader changes within the etcd3 cluster are happening

  ### GRPC requests alerts ###
  - record: etcd3:failed_client_requests_ratio:sum
    expr: sum by(grpc_method) (rate(etcd_grpc_requests_failed_total{job="kube-etcd3"}[5m]))
      / sum by(grpc_method) (rate(etcd_grpc_total{job="kube-etcd3"}[5m]))

  # alert if more than 1% of requests to GRPC have failed
  - alert: KubeEtcd3HighNumberOfFailedGRPCRequests
    expr: etcd3:failed_client_requests_ratio:sum > 0.01
    for: 10m
    labels:
      service: etcd
      severity: warning
      type: seed
    annotations:
      description: '{{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd3 pod {{ $labels.pod }}'
      summary: a high number of etcd3 GRPC requests are failing

  # alert if more than 5% of requests to GRPC have failed
  - alert: KubeEtcd3HighNumberOfFailedGRPCRequests
    expr: etcd3:failed_client_requests_ratio:sum > 0.05
    for: 5m
    labels:
      service: etcd
      severity: critical
      type: seed
    annotations:
      description: '{{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd3 pod {{ $labels.pod }}'
      summary: a high number of etcd3 GRPC requests are failing

  # alert if the 99th percentile of GRPC requests take more than 150ms
  # etcd3 must be configured with --metrics="extensive" for histogram
  - alert: KubeEtcd3GRPCRequestsSlow
    expr: histogram_quantile(0.99, rate(etcd_grpc_unary_requests_duration_seconds_bucket[5m])) > 0.15
    for: 10m
    labels:
      service: etcd
      severity: warning
      type: seed
    annotations:
      description: on ectd3 instance {{ $labels.instance }} GRPC requests to {{ $label.grpc_method }} are slow
      summary: slow GRPC requests

  ### etcd proposal alerts ###

  # alert if there are several failed proposals within an hour
  - alert: KubeEtcd3HighNumberOfFailedProposals
    expr: increase(etcd_server_proposals_failed_total{job="kube-etcd3"}[1h]) > 5
    labels:
      service: etcd
      severity: warning
      type: seed
    annotations:
      description: etcd3 pod {{ $labels.pod }} has seen {{ $value }} proposal failures
        within the last hour
      summary: a high number of failed proposals within the etcd cluster are happening

  ### etcd disk io latency alerts ###

  # alert if 99th percentile of fsync durations is higher than 500ms
  - alert: KubeEtcd3HighFsyncDurations
    expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) > 0.5
    for: 10m
    labels:
      service: etcd
      severity: warning
      type: seed
    annotations:
      description: ectd3 pod {{ $labels.pod }} fync durations are high
      summary: high fsync durations

  # alert if 99th percentile of commit durations is higher than 250ms
  - alert: KubeEtcd3HighCommitDurations
    expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job="kube-etcd3"}[5m]))
      > 0.25
    for: 10m
    labels:
      service: etcd
      severity: warning
      type: seed
    annotations:
      description: etcd3 pod {{ $labels.pod }} commit durations are high
      summary: high commit durations

  # etcd member communication alerts
  # ================================

  # alert if 99th percentile of round trips take 150ms
  - alert: KubeEtcd3MemberCommunicationSlow
    expr: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job="kube-etcd3"}[5m]))
      > 0.15
    for: 10m
    labels:
      service: etcd
      severity: warning
      type: seed
    annotations:
      description: etcd3 pod {{ $labels.pod }} member communication with {{ $label.To
        }} is slow
      summary: etcd member communication is slow

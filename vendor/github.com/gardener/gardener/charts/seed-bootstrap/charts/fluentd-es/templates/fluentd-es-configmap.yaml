apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-es-config
  namespace: {{ .Release.Namespace }}
  labels:
{{ toYaml .Values.fluentd.labels | indent 4 }}
data:
  input.conf: |-
    # Takes the messages sent over TCP
    <source>
      @type forward
      @log_level warn
      port 24224
      bind 0.0.0.0
    </source>

  output.conf: |-
    
    #Decide which is shoot record and which is not
    <match kubernetes.**>
      @id rewrite_tag_filter
      @type rewrite_tag_filter
      @log_level warn
      <rule>
        key $.kubernetes.namespace_name
        pattern ^(shoot.*)
        tag $1
      </rule>
      <rule>
        key $.kubernetes.namespace_name
        pattern ^shoot.*
        invert true
        tag seed
      </rule>
    </match>

    <filter **>
      @type elasticsearch_genid
    </filter>

    <match shoot**>
      @id elasticsearch_dynamic
      @type elasticsearch_dynamic
      @log_level warn
      host elasticsearch-logging.${record['kubernetes']['namespace_name']}.svc
      @include /etc/fluent/config.d/es_plugin.options
      <buffer tag, time>
        @type file
        #Limit the number of queued chunks
        queued_chunks_limit_size 4096
        # The number of threads of output plugins, which is used to write chunks in parallel
        flush_thread_count 32
        # he size limitation of this buffer plugin instance
        total_limit_size 6GB
        @include /etc/fluent/config.d/buffer.options
        path /gardener/fluentd-buffers/kubernetes.shoot.buffer
      </buffer>
      # avoiding backup of unrecoverble errors
      <secondary>
        @type null
      </secondary>
    </match>

    <match **>
      @id elasticsearch_static
      @type elasticsearch
      @log_level warn
      host elasticsearch-logging.{{ .Release.Namespace }}.svc
      @include /etc/fluent/config.d/es_plugin.options
      template_name gardener
      template_file /etc/fluent/config.d/default.template
      <buffer tag, time>
        @type file
        #Limit the number of queued chunks
        queued_chunks_limit_size 40
        # The number of threads of output plugins, which is used to write chunks in parallel
        flush_thread_count 8
        # he size limitation of this buffer plugin instance
        total_limit_size 2GB
        @include /etc/fluent/config.d/buffer.options
        path /gardener/fluentd-buffers/kubernetes.{{ .Release.Namespace }}.buffer
      </buffer>
      <secondary>
        @type null
      </secondary>
    </match>

  system.conf: |-
    <system>
      root_dir /gardener/fluentd-buffers/
    </system>

  es_plugin.options: |-
    port {{ .Values.global.elasticsearchPorts.db }}
    logstash_format true
    # use _hash as request id
    id_key _hash
    reconnect_on_error true
    # gives enough time fluentd to read the response
    request_timeout 30s
    # this is only for debug. When flush take more time than this it will produce warning
    slow_flush_log_threshold 60
    # Do not wait ES to tell you what version is.
    verify_es_version_at_startup false
    # just hardcode that version number
    default_elasticsearch_version 6
    # we have time field from the docker runtime
    # include_timestamp false
    time_key time
    # remove tag from the log content
    include_tag_key false
    remove_keys _hash, time

  buffer.options: |-
    # The max size of each chunks
    chunk_limit_size 50MB
    #chunks per 5m
    timekey 300
    # delay for flush
    timekey_wait 0
    # The percentage of chunk size threshold for flushing
    chunk_full_threshold 0.9
    # no compression
    compress text
    # flush/write all buffer chunks at shutdown
    flush_at_shutdown true
    # flush/write chunks per specified time
    flush_interval 60s
    # The sleep interval seconds of threads to wait next flush trial (when no chunks are waiting)
    flush_thread_interval 30.0
    # How output plugin behaves when its buffer queue is full
    overflow_action drop_oldest_chunk
    # output plugin will retry periodically with fixed intervals (configured via retry_wait)
    retry_type periodic
    # Seconds to wait before next retry to flush
    retry_wait 75
    retry_randomize false
    flush_mode interval
    retry_max_times 4

  default.template: |-
    {
      "index_patterns" : ["logstash-*"],
      "settings" : {
        "index" :{
          "number_of_shards" : "3",
          "number_of_replicas" : "0",
          "refresh_interval": "60s",
          "translog":{
            "durability": "async"
          }
        }
      }
    }
